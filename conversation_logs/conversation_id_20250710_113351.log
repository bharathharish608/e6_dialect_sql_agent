{
  "turn": 1,
  "conversation_id": "conversation_id_20250710_113351",
  "user_input": "is e6data a lakehouse compute engine?",
  "agent_output": "Yes, e6data is indeed a lakehouse compute engine. According to the official documentation, **e6data is a lakehouse compute engine built to run high concurrency, complex SQL analytics and AI workloads\u201410x faster, 60% cheaper, zero data movement** (Source: https://docs.e6data.com/product-documentation).\n\nThe platform is specifically **designed for data analysts, data scientists, and data engineers to get the best performance from their data infrastructure** (Source: https://docs.e6data.com/product-documentation).\n\nKey characteristics of e6data as a lakehouse compute engine include:\n- **High concurrency support** for multiple users and queries\n- **Complex SQL analytics capabilities** \n- **AI workload support**\n- **Performance optimization** (10x faster than alternatives)\n- **Cost efficiency** (60% cheaper)\n- **Zero data movement** architecture for security and performance\n\nThe documentation confirms that e6data serves as the compute layer for lakehouse architectures, enabling fast analytics and AI workloads directly on your data without requiring data movement.",
  "timestamp": "2025-07-10T11:34:28.760459"
}
{
  "turn": 2,
  "conversation_id": "conversation_id_20250710_113351",
  "user_input": "does it support running generative ai workloads like finetuning?",
  "agent_output": "I need to search for information about e6data's support for generative AI workloads and finetuning capabilities.\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">generative AI workloads finetuning machine learning model training</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">AI workloads machine learning ML capabilities</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\nBased on the available documentation, I can see that e6data is described as supporting \"AI workloads\" in general terms. According to the documentation, **e6data is a lakehouse compute engine built to run high concurrency, complex SQL analytics and AI workloads** (Source: https://docs.e6data.com/product-documentation).\n\nHowever, the documentation does not provide specific details about what types of AI workloads are supported or whether generative AI workloads like finetuning are specifically supported. The available context mentions \"AI workloads\" but doesn't elaborate on the specific capabilities, frameworks, or types of AI/ML operations that e6data can handle.\n\nI don't have enough information in the provided documentation to definitively answer whether e6data supports generative AI workloads like finetuning. You would need to consult with the e6data team directly or refer to more detailed technical documentation about their AI/ML capabilities.",
  "timestamp": "2025-07-10T11:35:30.798174"
}
{
  "turn": 3,
  "conversation_id": "conversation_id_20250710_113351",
  "user_input": "how does e6data achieve 60% better performance compared to other w",
  "agent_output": "I need to search for information about how e6data achieves its performance advantages and cost savings.\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">e6data performance optimization cost savings architecture shuffle elimination</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">e6data principles efficiency network shuffle vectorized execution</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\nBased on the available documentation, e6data achieves its performance advantages (10x faster) and cost benefits (60% cheaper) through several key principles:\n\n## **Eliminate the Brute Force Approach**\n**e6data creates multiple layers of indirection that enable it to reduce the volume of data operated on by orders of magnitude. This reduces the amount of network shuffle involved in large queries by > 95%, thereby eliminating the cause of multiplicative delays** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\n## **Foresight-Based, Vectorized Query Evaluation & Execution**\n**e6data's engine evaluates a query in its entirety and intelligently combines the execution of multiple heavy operations into a single stage. Independent execution branches are kicked off concurrently for highly parallelized execution** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\n## **Consensus-Based Distribution Framework**\n**Unlike Spark and Presto/Trino, e6data's consensus-based distribution framework does not have the coordinator/driver as a single point of failure. Nor does it trade-off between fault tolerance and low latency** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\n## **Kubernetes Native Architecture**\n**Being Kubernetes native means e6data supports multiple environments and allows e6data to efficiently auto-scale to meet demanding SLAs. Coupled with our obsessive focus on shuffle elimination, performance gains increase with increases in data volumes and query complexity** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\nThe key differentiator is e6data's **\"obsessive focus on shuffle elimination\"** which reduces network overhead by over 95%, leading to both better performance and lower costs.",
  "timestamp": "2025-07-10T11:36:38.507812"
}
{
  "turn": 4,
  "conversation_id": "conversation_id_20250710_113351",
  "user_input": "is it easy to deploy on aws? what are the steps to deploy?",
  "agent_output": "I need to search for specific information about e6data's AWS deployment process and steps.\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">AWS deployment steps installation guide setup process</parameter>\n</invoke>\n</function_calls>\n<function_result>\nSource: https://docs.e6data.com/product-documentation/setup/aws-setup\nSetup AWS Setup The page provides setup guides for deploying e6data on AWS. AWS Setup Guide Get started with our AWS setup by choosing you model that best fits your needs. In VPC Serverless Select your preferred model to access the step-by-step setup guide. Previous Setup Next In VPC Deployment (AWS) Last updated 4 months ago\n\nSource: https://docs.e6data.com/product-documentation/setup\nSetup Setting up your cloud for e6data This page provides information about the setup of different cloud types supported: AWS Setup In VPC Deployment Connect to Serverless Compute GCP Setup In VPC Deployment Connect to Serverless Compute : Coming Soon \ud83d\udd1c Azure Setup (Beta) Previous Sign Up Next AWS Setup Last updated 4 months ago\n\nSource: https://docs.e6data.com/product-documentation/services-utilised-for-e6data-deployment\nServices Utilised for e6data Deployment This page outlines the key services and resources required for deploying e6data on various cloud platforms. The following are the services used for e6data deployment, along with the corresponding regions where these services are available. AWS Supported Regions GCP Supported Regions AZURE Supported Regions Previous Catalog Privileges Next AWS supported regions Last updated 10 months ago\n\nSource: https://docs.e6data.com/product-documentation/faqs/security\nFAQs Security Frequently Asked Questions about Security What access controls are in place and how can they be audited? All Terraform scripts used to deploy e6data can be found in the e6data documentation: Creating Workspaces in AWS The AWS Access Policies created by Terraform can be found here: Workspace Creation Permissions View the policies on GitHub All permissions are created using the IAM OIDC method for the particular Kubernetes service account in the isolated e6data Kubernetes namespace in the EKS cluster. Summary of Required Permissions The e6data query engine cluster requires read-only access to the data to be queried (S3 buckets) and metastore/catalogs. During workspace deployment, customers are asked to specify which S3 buckets should be accessible the to query engine (i.e bucket level and prefix level) depending on the use cases required. These specific lines in the Terraform script define S3 access: https://github.com/e6x-labs/terraform/blob/main/aws/e6data_engine_iam.tf#L19-L50 A new S3 bucket for e6data use is created with read-write access in the customer account. It is used to store the following: Schema metadata (table names, column names, number of rows, file paths) for increased performance. If the Query Editor is used, Sheets (SQL query text) are stored. Read only access to AWS Glue is required for the engine to know the up-to-date paths to data for efficient query planning compute use. How are e6data personnel restricted from accessing customers' data? How can this be audited? Limited Access to e6data Control Plane e6data employees do not have access to the e6data production system (e6data Control Plane). e6data employees can only monitor & debug the Control Plane using Grafana. e6data employees can only perform deployments via a CI/CD pipeline secured with multi-factor authentication. No Access to Data Plane e6data employees have no way of accessing the Data Plane in the customer's environment. All communication between the Control Plane & Data Plane takes place between applications. Multiple levels of logging are in place to audit inter-planar communication: Kubernetes logging It is recommended to enable the Audit and Authenticator logging features in your EKS cluster which helps monitor the operations performed by the e6data Kubernetes user and any IAM authentications for Kubernetes entry points that take place. Guide to enabling Kubernetes logging S3 access logging The S3 bucket created during deployment for the e6data query engine's usage will have access logs enabled by default. This helps to track the access usage by e6data Control Plane applications on the mentioned S3 bucket. Guide to enabling S3 access logging Any API call that can cause the query engine to resume, suspend or scale needs to pass through the IAM layer to make an STS call on the Assume Role. This can be logged by the customer using Cloudtrail to detect any suspicious events. Guide to integrating Cloudtrail EKS Authorized Networks The IP range of e6data Control Plane can be allow listed for EKS access in addition to the IAM and Kubernetes RBAC access controls as an added layer of security Guide to using Authorized Networks for EKS What data moves outside the customer's data plane? What data is stored in the Control Plane? The only data that moves from the Data Plane to the Control Plane and are stored by e6data are: Usage, logs, and infrastructure metrics Cluster infrastructure metrics Number of pods and how long pods of each component (planners, queue managers, executors) are running How many nodes are active How long nodes are active for CPU, memory, network, and I/O utilization of each pod Cluster query metrics Count of count successful, failed & running queries. Masked query (SQL) text Any part that may contain sensitive information is masked prior to movement to the control plane (filter by, group by, where clauses) Hash of queries that are run. The run duration of each query. User who ran the query. Schema information Schemas (table and column names) for Schema Explorer in Query Editor What data other data moves outside the Data Plane? If the Query Editor is used, query results will be streamed to the user's browser via the Control Plane. However, Query results are only visible in UI. Data remains in your data plane \u2013 e6data doesn't upload, access, or download it. If you use the Query Editor your query is created on the front end of e6data's web UI in your browser, behind your firewalls and VPN, and is not accessed by any e6data employees.\n</function_result>\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">terraform deployment workspace creation permissions EKS cluster</parameter>\n</invoke>\n</function_calls>\n<function_result>\nSource: https://docs.e6data.com/product-documentation/faqs/security\nFAQs Security Frequently Asked Questions about Security What access controls are in place and how can they be audited? All Terraform scripts used to deploy e6data can be found in the e6data documentation: Creating Workspaces in AWS The AWS Access Policies created by Terraform can be found here: Workspace Creation Permissions View the policies on GitHub All permissions are created using the IAM OIDC method for the particular Kubernetes service account in the isolated e6data Kubernetes namespace in the EKS cluster. Summary of Required Permissions The e6data query engine cluster requires read-only access to the data to be queried (S3 buckets) and metastore/catalogs. During workspace deployment, customers are asked to specify which S3 buckets should be accessible the to query engine (i.e bucket level and prefix level) depending on the use cases required. These specific lines in the Terraform script define S3 access: https://github.com/e6x-labs/terraform/blob/main/aws/e6data_engine_iam.tf#L19-L50 A new S3 bucket for e6data use is created with read-write access in the customer account. It is used to store the following: Schema metadata (table names, column names, number of rows, file paths) for increased performance. If the Query Editor is used, Sheets (SQL query text) are stored. Read only access to AWS Glue is required for the engine to know the up-to-date paths to data for efficient query planning compute use. How are e6data personnel restricted from accessing customers' data? How can this be audited? Limited Access to e6data Control Plane e6data employees do not have access to the e6data production system (e6data Control Plane). e6data employees can only monitor & debug the Control Plane using Grafana. e6data employees can only perform deployments via a CI/CD pipeline secured with multi-factor authentication. No Access to Data Plane e6data employees have no way of accessing the Data Plane in the customer's environment. All communication between the Control Plane & Data Plane takes place between applications. Multiple levels of logging are in place to audit inter-planar communication: Kubernetes logging It is recommended to enable the Audit and Authenticator logging features in your EKS cluster which helps monitor the operations performed by the e6data Kubernetes user and any IAM authentications for Kubernetes entry points that take place. Guide to enabling Kubernetes logging S3 access logging The S3 bucket created during deployment for the e6data query engine's usage will have access logs enabled by default. This helps to track the access usage by e6data Control Plane applications on the mentioned S3 bucket. Guide to enabling S3 access logging Any API call that can cause the query engine to resume, suspend or scale needs to pass through the IAM layer to make an STS call on the Assume Role. This can be logged by the customer using Cloudtrail to detect any suspicious events. Guide to integrating Cloudtrail EKS Authorized Networks The IP range of e6data Control Plane can be allow listed for EKS access in addition to the IAM and Kubernetes RBAC access controls as an added layer of security Guide to using Authorized Networks for EKS What data moves outside the customer's data plane? What data is stored in the Control Plane? The only data that moves from the Data Plane to the Control Plane and are stored by e6data are: Usage, logs, and infrastructure metrics Cluster infrastructure metrics Number of pods and how long pods of each component (planners, queue managers, executors) are running How many nodes are active How long nodes are active for CPU, memory, network, and I/O utilization of each pod Cluster query metrics Count of count successful, failed & running queries. Masked query (SQL) text Any part that may contain sensitive information is masked prior to movement to the control plane (filter by, group by, where clauses) Hash of queries that are run. The run duration of each query. User who ran the query. Schema information Schemas (table and column names) for Schema Explorer in Query Editor What data other data moves outside the Data Plane? If the Query Editor is used, query results will be streamed to the user's browser via the Control Plane. However, Query results are only visible in UI. Data remains in your data plane \u2013 e6data doesn't upload, access, or download it. If you use the Query Editor your query is created on the front end of e6data's web UI in your browser, behind your firewalls and VPN, and is not accessed by any e6data employees.\n</function_result>\n\nYes, e6data is designed to be deployed on AWS and appears to have a structured approach to make deployment manageable. Based on the documentation, here's what I can tell you about AWS deployment:\n\n## **AWS Deployment Options**\n\ne6data offers **two main deployment models on AWS** (Source: https://docs.e6data.com/product-documentation/setup/aws-setup):\n\n1. **In VPC Deployment** \n2. **Serverless**\n\nYou can **\"choose your model that best fits your needs\"** and **\"access the step-by-step setup guide\"** (Source: https://docs.e6data.com/product-documentation/setup/aws-setup).\n\n## **Deployment Infrastructure**\n\nThe deployment uses **Terraform scripts** for automation. **All Terraform scripts used to deploy e6data can be found in the e6data documentation** (Source: https://docs.e6data.com/product-documentation/faqs/security).\n\n## **Key AWS Services Required**\n\nBased on the documentation, the deployment involves:\n\n- **EKS Cluster**: **All permissions are created using the IAM OIDC method for the particular Kubernetes service account in the isolated e6data Kubernetes namespace in the EKS cluster** (Source: https://docs.e6data.com/product-documentation/faqs/security)\n\n- **S3 Buckets**: **A new S3 bucket for e6data use is created with read-write access in the customer account** to store schema metadata and query sheets (Source: https://docs.e6data.com/product-documentation/faqs/security)\n\n- **AWS Glue**: **Read only access to AWS Glue is required for the engine to know the up-to-date paths to data for efficient query planning** (Source: https://docs.e6data.com/product-documentation/faqs/security)\n\n## **Security and Permissions**\n\nThe deployment process includes:\n- **IAM roles and policies** managed through Terraform\n- **Bucket-level and prefix-level access controls** for S3\n- **Integration with AWS CloudTrail** for auditing\n\nHowever, the documentation doesn't provide the detailed step-by-step installation process in the context provided. For specific deployment steps, you would need to access the complete AWS setup guide referenced in the documentation at https://docs.e6data.com/product-documentation/setup/aws-setup.",
  "timestamp": "2025-07-10T11:39:26.649395"
}

--- Conversation Ended ---
{
  "final_conversation_history": [
    {
      "role": "user",
      "content": "is e6data a lakehouse compute engine?"
    },
    {
      "role": "agent",
      "content": "Yes, e6data is indeed a lakehouse compute engine. According to the official documentation, **e6data is a lakehouse compute engine built to run high concurrency, complex SQL analytics and AI workloads\u201410x faster, 60% cheaper, zero data movement** (Source: https://docs.e6data.com/product-documentation).\n\nThe platform is specifically **designed for data analysts, data scientists, and data engineers to get the best performance from their data infrastructure** (Source: https://docs.e6data.com/product-documentation).\n\nKey characteristics of e6data as a lakehouse compute engine include:\n- **High concurrency support** for multiple users and queries\n- **Complex SQL analytics capabilities** \n- **AI workload support**\n- **Performance optimization** (10x faster than alternatives)\n- **Cost efficiency** (60% cheaper)\n- **Zero data movement** architecture for security and performance\n\nThe documentation confirms that e6data serves as the compute layer for lakehouse architectures, enabling fast analytics and AI workloads directly on your data without requiring data movement."
    },
    {
      "role": "user",
      "content": "does it support running generative ai workloads like finetuning?"
    },
    {
      "role": "agent",
      "content": "I need to search for information about e6data's support for generative AI workloads and finetuning capabilities.\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">generative AI workloads finetuning machine learning model training</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">AI workloads machine learning ML capabilities</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\nBased on the available documentation, I can see that e6data is described as supporting \"AI workloads\" in general terms. According to the documentation, **e6data is a lakehouse compute engine built to run high concurrency, complex SQL analytics and AI workloads** (Source: https://docs.e6data.com/product-documentation).\n\nHowever, the documentation does not provide specific details about what types of AI workloads are supported or whether generative AI workloads like finetuning are specifically supported. The available context mentions \"AI workloads\" but doesn't elaborate on the specific capabilities, frameworks, or types of AI/ML operations that e6data can handle.\n\nI don't have enough information in the provided documentation to definitively answer whether e6data supports generative AI workloads like finetuning. You would need to consult with the e6data team directly or refer to more detailed technical documentation about their AI/ML capabilities."
    },
    {
      "role": "user",
      "content": "how does e6data achieve 60% better performance compared to other w"
    },
    {
      "role": "agent",
      "content": "I need to search for information about how e6data achieves its performance advantages and cost savings.\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">e6data performance optimization cost savings architecture shuffle elimination</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">e6data principles efficiency network shuffle vectorized execution</parameter>\n</invoke>\n</function_calls>\n<function_result>\nNo relevant results found for the query.\n</function_result>\n\nBased on the available documentation, e6data achieves its performance advantages (10x faster) and cost benefits (60% cheaper) through several key principles:\n\n## **Eliminate the Brute Force Approach**\n**e6data creates multiple layers of indirection that enable it to reduce the volume of data operated on by orders of magnitude. This reduces the amount of network shuffle involved in large queries by > 95%, thereby eliminating the cause of multiplicative delays** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\n## **Foresight-Based, Vectorized Query Evaluation & Execution**\n**e6data's engine evaluates a query in its entirety and intelligently combines the execution of multiple heavy operations into a single stage. Independent execution branches are kicked off concurrently for highly parallelized execution** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\n## **Consensus-Based Distribution Framework**\n**Unlike Spark and Presto/Trino, e6data's consensus-based distribution framework does not have the coordinator/driver as a single point of failure. Nor does it trade-off between fault tolerance and low latency** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\n## **Kubernetes Native Architecture**\n**Being Kubernetes native means e6data supports multiple environments and allows e6data to efficiently auto-scale to meet demanding SLAs. Coupled with our obsessive focus on shuffle elimination, performance gains increase with increases in data volumes and query complexity** (Source: https://docs.e6data.com/product-documentation/introduction-to-e6data).\n\nThe key differentiator is e6data's **\"obsessive focus on shuffle elimination\"** which reduces network overhead by over 95%, leading to both better performance and lower costs."
    },
    {
      "role": "user",
      "content": "is it easy to deploy on aws? what are the steps to deploy?"
    },
    {
      "role": "agent",
      "content": "I need to search for specific information about e6data's AWS deployment process and steps.\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">AWS deployment steps installation guide setup process</parameter>\n</invoke>\n</function_calls>\n<function_result>\nSource: https://docs.e6data.com/product-documentation/setup/aws-setup\nSetup AWS Setup The page provides setup guides for deploying e6data on AWS. AWS Setup Guide Get started with our AWS setup by choosing you model that best fits your needs. In VPC Serverless Select your preferred model to access the step-by-step setup guide. Previous Setup Next In VPC Deployment (AWS) Last updated 4 months ago\n\nSource: https://docs.e6data.com/product-documentation/setup\nSetup Setting up your cloud for e6data This page provides information about the setup of different cloud types supported: AWS Setup In VPC Deployment Connect to Serverless Compute GCP Setup In VPC Deployment Connect to Serverless Compute : Coming Soon \ud83d\udd1c Azure Setup (Beta) Previous Sign Up Next AWS Setup Last updated 4 months ago\n\nSource: https://docs.e6data.com/product-documentation/services-utilised-for-e6data-deployment\nServices Utilised for e6data Deployment This page outlines the key services and resources required for deploying e6data on various cloud platforms. The following are the services used for e6data deployment, along with the corresponding regions where these services are available. AWS Supported Regions GCP Supported Regions AZURE Supported Regions Previous Catalog Privileges Next AWS supported regions Last updated 10 months ago\n\nSource: https://docs.e6data.com/product-documentation/faqs/security\nFAQs Security Frequently Asked Questions about Security What access controls are in place and how can they be audited? All Terraform scripts used to deploy e6data can be found in the e6data documentation: Creating Workspaces in AWS The AWS Access Policies created by Terraform can be found here: Workspace Creation Permissions View the policies on GitHub All permissions are created using the IAM OIDC method for the particular Kubernetes service account in the isolated e6data Kubernetes namespace in the EKS cluster. Summary of Required Permissions The e6data query engine cluster requires read-only access to the data to be queried (S3 buckets) and metastore/catalogs. During workspace deployment, customers are asked to specify which S3 buckets should be accessible the to query engine (i.e bucket level and prefix level) depending on the use cases required. These specific lines in the Terraform script define S3 access: https://github.com/e6x-labs/terraform/blob/main/aws/e6data_engine_iam.tf#L19-L50 A new S3 bucket for e6data use is created with read-write access in the customer account. It is used to store the following: Schema metadata (table names, column names, number of rows, file paths) for increased performance. If the Query Editor is used, Sheets (SQL query text) are stored. Read only access to AWS Glue is required for the engine to know the up-to-date paths to data for efficient query planning compute use. How are e6data personnel restricted from accessing customers' data? How can this be audited? Limited Access to e6data Control Plane e6data employees do not have access to the e6data production system (e6data Control Plane). e6data employees can only monitor & debug the Control Plane using Grafana. e6data employees can only perform deployments via a CI/CD pipeline secured with multi-factor authentication. No Access to Data Plane e6data employees have no way of accessing the Data Plane in the customer's environment. All communication between the Control Plane & Data Plane takes place between applications. Multiple levels of logging are in place to audit inter-planar communication: Kubernetes logging It is recommended to enable the Audit and Authenticator logging features in your EKS cluster which helps monitor the operations performed by the e6data Kubernetes user and any IAM authentications for Kubernetes entry points that take place. Guide to enabling Kubernetes logging S3 access logging The S3 bucket created during deployment for the e6data query engine's usage will have access logs enabled by default. This helps to track the access usage by e6data Control Plane applications on the mentioned S3 bucket. Guide to enabling S3 access logging Any API call that can cause the query engine to resume, suspend or scale needs to pass through the IAM layer to make an STS call on the Assume Role. This can be logged by the customer using Cloudtrail to detect any suspicious events. Guide to integrating Cloudtrail EKS Authorized Networks The IP range of e6data Control Plane can be allow listed for EKS access in addition to the IAM and Kubernetes RBAC access controls as an added layer of security Guide to using Authorized Networks for EKS What data moves outside the customer's data plane? What data is stored in the Control Plane? The only data that moves from the Data Plane to the Control Plane and are stored by e6data are: Usage, logs, and infrastructure metrics Cluster infrastructure metrics Number of pods and how long pods of each component (planners, queue managers, executors) are running How many nodes are active How long nodes are active for CPU, memory, network, and I/O utilization of each pod Cluster query metrics Count of count successful, failed & running queries. Masked query (SQL) text Any part that may contain sensitive information is masked prior to movement to the control plane (filter by, group by, where clauses) Hash of queries that are run. The run duration of each query. User who ran the query. Schema information Schemas (table and column names) for Schema Explorer in Query Editor What data other data moves outside the Data Plane? If the Query Editor is used, query results will be streamed to the user's browser via the Control Plane. However, Query results are only visible in UI. Data remains in your data plane \u2013 e6data doesn't upload, access, or download it. If you use the Query Editor your query is created on the front end of e6data's web UI in your browser, behind your firewalls and VPN, and is not accessed by any e6data employees.\n</function_result>\n\n<function_calls>\n<invoke name=\"vector_search_tool\">\n<parameter name=\"query\">terraform deployment workspace creation permissions EKS cluster</parameter>\n</invoke>\n</function_calls>\n<function_result>\nSource: https://docs.e6data.com/product-documentation/faqs/security\nFAQs Security Frequently Asked Questions about Security What access controls are in place and how can they be audited? All Terraform scripts used to deploy e6data can be found in the e6data documentation: Creating Workspaces in AWS The AWS Access Policies created by Terraform can be found here: Workspace Creation Permissions View the policies on GitHub All permissions are created using the IAM OIDC method for the particular Kubernetes service account in the isolated e6data Kubernetes namespace in the EKS cluster. Summary of Required Permissions The e6data query engine cluster requires read-only access to the data to be queried (S3 buckets) and metastore/catalogs. During workspace deployment, customers are asked to specify which S3 buckets should be accessible the to query engine (i.e bucket level and prefix level) depending on the use cases required. These specific lines in the Terraform script define S3 access: https://github.com/e6x-labs/terraform/blob/main/aws/e6data_engine_iam.tf#L19-L50 A new S3 bucket for e6data use is created with read-write access in the customer account. It is used to store the following: Schema metadata (table names, column names, number of rows, file paths) for increased performance. If the Query Editor is used, Sheets (SQL query text) are stored. Read only access to AWS Glue is required for the engine to know the up-to-date paths to data for efficient query planning compute use. How are e6data personnel restricted from accessing customers' data? How can this be audited? Limited Access to e6data Control Plane e6data employees do not have access to the e6data production system (e6data Control Plane). e6data employees can only monitor & debug the Control Plane using Grafana. e6data employees can only perform deployments via a CI/CD pipeline secured with multi-factor authentication. No Access to Data Plane e6data employees have no way of accessing the Data Plane in the customer's environment. All communication between the Control Plane & Data Plane takes place between applications. Multiple levels of logging are in place to audit inter-planar communication: Kubernetes logging It is recommended to enable the Audit and Authenticator logging features in your EKS cluster which helps monitor the operations performed by the e6data Kubernetes user and any IAM authentications for Kubernetes entry points that take place. Guide to enabling Kubernetes logging S3 access logging The S3 bucket created during deployment for the e6data query engine's usage will have access logs enabled by default. This helps to track the access usage by e6data Control Plane applications on the mentioned S3 bucket. Guide to enabling S3 access logging Any API call that can cause the query engine to resume, suspend or scale needs to pass through the IAM layer to make an STS call on the Assume Role. This can be logged by the customer using Cloudtrail to detect any suspicious events. Guide to integrating Cloudtrail EKS Authorized Networks The IP range of e6data Control Plane can be allow listed for EKS access in addition to the IAM and Kubernetes RBAC access controls as an added layer of security Guide to using Authorized Networks for EKS What data moves outside the customer's data plane? What data is stored in the Control Plane? The only data that moves from the Data Plane to the Control Plane and are stored by e6data are: Usage, logs, and infrastructure metrics Cluster infrastructure metrics Number of pods and how long pods of each component (planners, queue managers, executors) are running How many nodes are active How long nodes are active for CPU, memory, network, and I/O utilization of each pod Cluster query metrics Count of count successful, failed & running queries. Masked query (SQL) text Any part that may contain sensitive information is masked prior to movement to the control plane (filter by, group by, where clauses) Hash of queries that are run. The run duration of each query. User who ran the query. Schema information Schemas (table and column names) for Schema Explorer in Query Editor What data other data moves outside the Data Plane? If the Query Editor is used, query results will be streamed to the user's browser via the Control Plane. However, Query results are only visible in UI. Data remains in your data plane \u2013 e6data doesn't upload, access, or download it. If you use the Query Editor your query is created on the front end of e6data's web UI in your browser, behind your firewalls and VPN, and is not accessed by any e6data employees.\n</function_result>\n\nYes, e6data is designed to be deployed on AWS and appears to have a structured approach to make deployment manageable. Based on the documentation, here's what I can tell you about AWS deployment:\n\n## **AWS Deployment Options**\n\ne6data offers **two main deployment models on AWS** (Source: https://docs.e6data.com/product-documentation/setup/aws-setup):\n\n1. **In VPC Deployment** \n2. **Serverless**\n\nYou can **\"choose your model that best fits your needs\"** and **\"access the step-by-step setup guide\"** (Source: https://docs.e6data.com/product-documentation/setup/aws-setup).\n\n## **Deployment Infrastructure**\n\nThe deployment uses **Terraform scripts** for automation. **All Terraform scripts used to deploy e6data can be found in the e6data documentation** (Source: https://docs.e6data.com/product-documentation/faqs/security).\n\n## **Key AWS Services Required**\n\nBased on the documentation, the deployment involves:\n\n- **EKS Cluster**: **All permissions are created using the IAM OIDC method for the particular Kubernetes service account in the isolated e6data Kubernetes namespace in the EKS cluster** (Source: https://docs.e6data.com/product-documentation/faqs/security)\n\n- **S3 Buckets**: **A new S3 bucket for e6data use is created with read-write access in the customer account** to store schema metadata and query sheets (Source: https://docs.e6data.com/product-documentation/faqs/security)\n\n- **AWS Glue**: **Read only access to AWS Glue is required for the engine to know the up-to-date paths to data for efficient query planning** (Source: https://docs.e6data.com/product-documentation/faqs/security)\n\n## **Security and Permissions**\n\nThe deployment process includes:\n- **IAM roles and policies** managed through Terraform\n- **Bucket-level and prefix-level access controls** for S3\n- **Integration with AWS CloudTrail** for auditing\n\nHowever, the documentation doesn't provide the detailed step-by-step installation process in the context provided. For specific deployment steps, you would need to access the complete AWS setup guide referenced in the documentation at https://docs.e6data.com/product-documentation/setup/aws-setup."
    }
  ]
}
